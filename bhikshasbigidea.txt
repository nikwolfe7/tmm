Given traning data -- an array of MFCCVectors for a particular class label,
e.g. GO or JUMP or BOTH

Training Data: X = x1 ... xT

For the entire set of MFCCVectors, run K-means to convergence, calculating
the mean centers of NUM_T_DISTRIBUTIONS different clusters. This will return a matrix
of mean values mu[], each with dim D for the size of the MFCCVectors.

1) Kmean(X,N)  -->  {mu[1..N]}, {It, t = 1..T} 

Returns:
It == the cluster index for a given vector (there will be T of these for T MFCCVectors)
mu[1...N] the means for the NUM_T_DISTRIBUTIONS clusters

This gives us back a set of means, each of dimension D for the size of the MFCCVectors. 
However, we believe that "GO" is a word represented by a mixture (NUM_T_DISTRIBUTIONS) of 
student t-Distributions, each with their individual parameters (mean[], var[], eta). 
For each t-Distribution, we associate a weight w to represent its contribution to the 
mixture. 

We therefore want to use these means[D] as a starting point to re-estimate both
the means themsleves, the variance, and the eta values. 

2) Initialize TMMs
	We initialize the means[D] to the values returned by te k-means algo
	Everything else will be set to some standard initialization. Etas will
	start as some improbably huge number, the mixture weights are uniform,
	and the variances are initialized to zero (though we will use the data
	to make a better estimate). we use count to estimate the variances. 
    
    # Initialize means, mixws and etas
    for n = 1 : N { // N is size of the T-mixture, NUM_T_DISTRIBUTIONS
        mean[n][D] = mu[n][D]   --- codewords returned by Kmeans
        mixw[n][D] = 1/N   --- Uniform
        eta[n] = 500    --- large number
        var[n][D] = 0   --- Initially set all to 0
        count[n] = 0 -- how many MFCC vectors are attributed to each component (N is size of NUM_T_DISTRIBUTIONS)
    }
    
    # We estimate the variances by taking the sum of the distances of each point (MFCCVector is a point)
    # in each cluster to the mean of that cluster and dividing by the count of the number of vectors 
    # belonging to each cluster.
    
    # Initialize variances as the variances of the clusters returned by Kmeans
    for t = 1 : T { // T is the number of MFCCVectors in the training data
    
    	# 'It' is the index of the cluster which the individual MFCCVector belongs to
    	# x[t] is the individual training vector
    	# mean[t] is the mean of the cluster (from K-means) pointed to by 'It'
    	# this is a variance vector for a SINGLE t-Distribution - of which we have NUM_T_DISTRIBUTIONS
    	
        var[n] += (x[t] - mean[It]) * (x[t] - mean[It])   --- Use the mean of the component indexed by It
        
        # this is just an array keeping track of the counts of vectors belonging to each group
        count[n] += 1   ---- Keep count -- this is how many vectors belong to each group
    }
    # Divide by the count for each index in the mixture...
    for n = 1 : N { // N is size of the T-mixture, NUM_T_DISTRIBUTIONS
    	# var[n] is the vector of dim D (size of MFCC)
        var[n] /= (count[t] - 1)  
    }
    
    The end result should be an array of TDistribution objects which is packed with the parameters 

3) EM!!!!!!!!!!!!!!!!!!!!!!!!
    for iter = 1 : NUMITER  { -------- We could use any termination condition, e.g. totallogprob must converge
        # Initialize various things
        mean_new[n=1..N] = 0,  var_new[n=1..N] = 0   ---- These are vectors
        mixw_new[n=1..N] = 0  --- This is scalar
        sumweight[1..N] = 0, etaconst[n=1..N] = 0    ---- Other variables we will use. These are not parameters

        # for updating the mean pt 1
        totallogprob = 0
        for t = 1 : T {# loop through all the training data
        	# logprob is the total log prob of the vector
            compute_posteriors(xt, TMM)   -->  logprob, posterior[n=1..N], u[1..N]  --- Note posterior and u are specific to x[t]
            totallogprob += logprob
            weight[n=1..N] = posterior[n] * u[n]  
            etaconst[n=1..N] += posterior[n] * (log(u[n]) - u[n])

            mixw_new[n=1..N] += posterior[n]
            mean_new[n=1..N] += weight[n] * x[t]  ---- Vector ops
            sumweight[n] += weight[n]

            # check that totallogprob has increased
            # update the logscaling constant in the TDistribution
        } 

        # remember that each element of mean_new[] or mean[] is the D dimensionality of your MFCC vectors in training data
        for n = 1 : N {# loop through the 64 T-distributions
            mean_new[n] = mean_new[n] / sumweight[n]
        }

        # remember that each element of var_new[] or var[] is the D dimensionality of your MFCC vectors in training data
        # for updating the variance 
        for t = 1 : T {# loop through the training data
            compute_posteriors(xt, TMM)   -->  posterior[n=1..N], u[1..N]  --- Note posterior and u are specific to x[t]
            weight[n=1..N] = posterior[n] * u[n]  
            var_new[n=1..N] += weight[n] * (x[t] - mean_new[n]) * (x[t] - mean_new[n])  ---- Vector ops (if you try this in the above loop it may be suboptimal but "should" be twice as fast -- but you would be using mean[n] NOT mean_new[n])
        }

        # updating mean pt 2 / variance, eta, mixture weights
        for n = 1 : N { # loop through the 64 T-distributions

            etaconst[n] /= mixw_new[n]

            mean[n] = mean_new[n]
            var[n] = var_new[n] / sumweight[n]
            # ALTERNATE FROM PEEL AND MCLACHLAN :   var[n] = var_new[n] / mixw_new[n]
            mixw[n] = mixw_new[n] / T

            eta[n] = solveforeta(etaconst[n], TMM[n])   ---- TMM[n] is the n-th T in the mixture
        }
    }


    # solve for eta
    solveforeta (etaconst, T) {
        etaconst = etaconst + 1 + digamma((T.eta + T.dim)/2) - log((T.eta + T.dim)/2)
        bsteta = 0;
        mincost = abs(etacost(bsteta, etaconst))
        for eta = 0 : 0.0001: 500 {       --- Need a real optimization algo here, this is a linear scan 
            thisetacost = abs(etacost(eta, etaconst))
            if (thisetacost < mincost){
                mincost = thisetacost
                bsteta = eta
            }
        }

        return bsteta
    }

    etacost(eta, etaconst){
        return log(eta/2) - digamma(eta/2)) + etaconst
    }